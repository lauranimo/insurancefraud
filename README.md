**Created this repo for easier sharing of this project**

## Problem and Dataset Description
Problem: The total cost of insurance fraud is more than $40 billion per year (FBI). Consequently, it is imperative for insurance companies to develop methods to discern legitimate claims from fraudulent claims. Processing these claims by hand is wholly unsustainable. Insurance companies receive millions of claims a year, and to individually inspect each claim is unfeasible. Consequently, it becomes necessary to create a machine-learning model to classify claims into legitimate and fraudulent categories. The model would be scalable, efficient, and introduce less bias in determining what claims are fraudulent.

Dataset Description: The dataset used to train the model is a collection of claims, demographic, vehicle, and policy information related to auto insurance claims. The data includes whether the customer claim was fraudulent, as well as an overview of the claimed collision. Information such as the number of cars involved, the type of collision, and the amount of damage are all documented, as well as information related to the demographics of the claimant and the features of the involved vehicle. The dataset has 46 total features, among which are the date of the incident, the type of collision, the geographical location of the collision, the number of witnesses, and the amount of damages the claimant filed. The dataset also contains descriptions of the vehicle involved, the policy of the claimant, and several demographic data points related to the claimant themselves. To limit complexity, however, we chose to focus on only the data contained in the claim, which has 20 features including the fraudulent status.
